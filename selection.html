---
layout: default
---

<div class="content">
    <h1>Background on DevConf 2023 Review Process</h1>
    <p>
    <p>
        We are excited to share with you how we selected the talks for the upcoming DevConf conference. Our goal is to
        be transparent and inclusive of our community, and to help you prepare for future submissions to DevConf and
        other events. We value your feedback and suggestions on how we can improve our event for you. Please email us at
        <a href="mailto:info@devconf.co.za">info@devconf.co.za</a> with any questions or comments.
    </p>
    <p>
        We used the <a href="https://sessionize.com/" target="_blank" rel="noopener noreferrer">Sessionize platform</a>
        to manage the talk submissions. You can still see the <a href="https://sessionize.com/devconf2023"
            target="_blank" rel="noopener noreferrer">call for speakers page</a> with all the details on our process. We
        use the <a href="https://sessionize.com/playbook/evaluation-modes-explained#comparison-evaluation-mode-2"
            target="_blank" rel="noopener noreferrer">Comparison evaluation mode</a> to compare and rank the talks.

        You can also try a free simulation on sessionize with 10 sample submissions to review and rank if you want to
        experience what we did.

    <p>
        We want to let you know that DevConf uses an anonymous review process. The reviewers only see the title, notes,
        description and track of each submission. Any other information you provide is hidden from them. Even we, as
        organisers, can’t see any extra information or personal details until the review process is over.
    </p>

    <p>
        We are grateful to our panel of 21 reviewers who ranked the talks for DevConf 2023 in Sessionize. They come from
        diverse backgrounds of race, gender, age, experience, technology and disabilities. They are all previous
        speakers or reviewers of DevConf and have a deep understanding of our conference’s vision and values. We did not
        give them any specific criteria or instructions - we trusted them to use their expertise and judgment to select
        the best talks. Reviewers had the option to provide written feedback on a submission, but most did not. We
        understand that they did this work voluntarily and generously in their spare time without any compensation. We
        appreciate their time and effort in reviewing the talks. The final ranking was determined by the average of the
        Elo scores assigned by the reviewers. The total time spent on reviewing talks was 35 515 minutes.
    </p>
    <p>
        Once that that was done, Candice and Robert, who are the organisers of the event, built the agenda mostly from
        the ordered list (i.e. talks ranked 1
        through 40 go in), with a few exceptions:
    <ul>
        <li>International speakers - Due to cost limits, once the budget for international speakers was reached all
            other international speakers were declined.</li>
        <li>A speaker with a selected talk would have all their other submissions dropped. This is to allow many diverse
            views and opportunities (one speaker sharing twice in a day vs. 2 speakers sharing once in a day) and
            allow speakers to spend time on the day with the audience and enjoy the event as well as to give them
            time to focus.</li>
        <li>Duplication - If we already have a talk that covers the same ground, we won't select a second one. We
            could have talks on the same topic if it's clear that there is distinction and value for the audience. </li>
        <li>Content that doesn't fit - We published a list of what is and isn't suitable content as part of the
            submission request
            and thankfully the ranking pushes these down so we have not had to drop any highly ranked inappropriate
            talks but that is a possibility.</li>
        <li>Discretionary - As organisers, we need to ensure a balanced and relevant agenda, based entirely on
            community feedback provided over the years so that the conference continues to be a high-quality event.
            We apply discretion only to ensure our agenda suits our audience's varying interests. We are guided by
            reviewers' feedback, community feedback on social media and at meetups and other conferences and this
            discretion is applied exceptionally sparingly. <br />
            A good example from this year is we had 2 similar talks in our top 20 in the 2021 reviews; the first talk
            was, in
            fact, the #1 ranked talk and was from an international speaker, the other was from a local speaker. We
            opted for the lower-ranked one as the local speaker had the advantage of relevance in our market
            (which was pertinant to the talk) and we were confident they would present and capture their audience just
            as well as the international speaker.</li>
        <li>Where there are similar talks that rank similarly, we would allow the opportunity to underrepresented
            groups.</li>
        <li>We also try to limit too much content in a particular theme - considering most of our submissions are
            about languages and tools, and many ranked in the top 12 - and this is to make room for
            talks in categories such as mental health, DevOps and Security, for example.</li>
    </ul>
    </p>
    <p>
    <h2>FAQ</h2>
    <h3>How many talks are there?</h3>
    We selected 29 talks from 622 submissions, meaning that only 4% of talks are selected.
    <h3>Does being International help?</h3>
    No. We believe the community in South Africa is amazing and brings a unique and powerful understanding that
    international speakers do not always have. In some years, DevConf has utilised its full budget for international
    speakers (which includes travel, accommodation and most meals) it is not a promise
    as seen in 2019 when the community did not rank international talks highly enough for selection.
    <h3>What can I do to increase my chances for selection in future?</h3>
    Write a great description. Your description needs to do a lot of work for you:
    <ul>
        <li>It needs to provide the detail of the content you are going to present potentially to a reviewer who
            isn't specifically an expert in it.</li>
        <li>It needs to show you know what you are talking about.</li>
        <li>It needs to have something to help it stand out. </li>

    </ul>
    This is difficult to do and talks a lot of writing and polishing to get it perfect. Attending the CFP
    workshops during the year that is run by the community is a very useful way to skill up here and this is why we
    partnered with other communities to run these for DevConf especially.
    A second tip is to engage in the community throughout the year - if you have an important topic, helping
    reviewers to know it is valuable ahead of conference time is invaluable.
    <h3>Does being a speaker from a current or previous sponsor help?</h3>
    Not at all. The process is anonymous so there is no way it can impact it. In addition, your event is run totally
    cost neutral with no profit being taken by the organisers so monatry influence is not possible.
    <h3>Why did the rejection emails not contain feedback?</h3>
    There are three factors that lead to a basic "sorry, you did not get selected email". <ul>
        <li>Sessionize has no way to automate giving even basic information (like rank for example) let alone
            feedback. </li>
        <li>As stated reviewers did not leave feedback for the majority of talks, so even if there was a solution
            there would be no content.</li>
        <li>It is not possible for Candice and Robert to write hundreds of personal rejections in a timeframe that is
            respectful of speakers who would wait months for feedback this way.</li>
    </ul>
    </p>
</div>